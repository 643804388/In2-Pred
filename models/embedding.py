from transformers import BertTokenizer, BertModel
from transformers import logging
import torch
from torch import nn

logging.set_verbosity_error()


class myBert(nn.Module):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    def __init__(self):
        super(myBert, self).__init__()
        # self.tokenizer = BertTokenizer.from_pretrained(r"F:\Plot_Reasoning\bert-base-chinese")
        self.tokenizer = BertTokenizer.from_pretrained(r"E:\科研项目\项目工程\动态图生成\bert-large-uncased")
        self.model = BertModel.from_pretrained(r"E:\科研项目\项目工程\动态图生成\bert-large-uncased").to(self.device)

    def forward(self, text):
        marked_text = "[CLS] " + text + " [SEP]"
        tokenized_text = self.tokenizer.tokenize(marked_text)
        # 将tokens字符串映射到其词汇索引vocabulary indices。
        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)
        segments_ids = [1] * len(tokenized_text)
        tokens_tensor = torch.tensor([indexed_tokens]).to(self.device)
        segments_tensors = torch.tensor([segments_ids]).to(self.device)
        self.model.eval()
        with torch.no_grad():
            outputs = self.model(tokens_tensor, segments_tensors)
        y = torch.mean(outputs[0], dim=1).to(self.device)
        return y, indexed_tokens[1]
